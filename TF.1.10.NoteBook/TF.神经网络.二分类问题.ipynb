{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练神经网络的过程大概可以分为以下三个步骤：\n",
    "# 1 定义神经网络的结构和前向传播的输出结果\n",
    "# 2 定义损失函数以及选择反响传播优化的算法\n",
    "# 3 生成会话并且在训练数据上反复运行反向传播优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练数据的大小\n",
    "batch_size = 64\n",
    "\n",
    "#定义神经网络的参数\n",
    "w1 = tf.Variable(tf.random_normal((2, 3), stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal((3, 1), stddev=1, seed=1))\n",
    "\n",
    "#在shape的一个维度使用None可以方便使用不同的batch大小。\n",
    "#在训练时需要把数据分成比较小的batch，但是在测试时，可以一次性使用全部的数据。\n",
    "#当数据集比较小时，这样比较方便测试，但数据集比较大时，将大量数据放入一个batch可能会导致内存溢出。\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "\n",
    "#定义神经网络的前向传播过程。\n",
    "#a = tf.matmul(x, w1)\n",
    "#y = tf.matmul(a, w2)\n",
    "\n",
    "#tf.nn.relu 为非线性激活函数\n",
    "a = tf.nn.relu(tf.matmul(x, w1), name=None)\n",
    "y = tf.nn.relu(tf.matmul(a, w2), name=None)\n",
    "\n",
    "#定义损失函数和反响传播的算法。\n",
    "y = tf.sigmoid(y)\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0))\n",
    "    +(1-y) * tf.log(tf.clip_by_value(1-y, 1e-10, 1.0))\n",
    ")\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过随机数生成一个模拟数据集。\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 12800000\n",
    "\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "#定义规则来给出样本的标签，在这里所有x1+x2<1的样例都被认为是正样本。其他为负样本。\n",
    "#大部分解决分类问题的神经网络都会采用0和1的表示方法来表示正样本和负样本。\n",
    "Y = [[int(x1+x2 < 1)] for (x1, x2) in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "0 0.5186172\n",
      "[[-0.8113182   1.5861262   0.18124548]\n",
      " [-2.4427042   0.20038475  0.7067059 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.5871149 ]\n",
      " [ 0.17046751]]\n"
     ]
    }
   ],
   "source": [
    "#创建一个会话来运行tensorflow程序。\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    #初始化变量\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    #训练前的神经网络的参数\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    #设定训练的轮数。\n",
    "    STEPS = 100\n",
    "    for i in range(STEPS):\n",
    "        #每次选取batch_size个样本进行训练。\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "        \n",
    "        #通过选取的样本训练神经网络并更新参数。\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 1000 == 0:\n",
    "            #每隔一段时间计算在所有数据上的交叉熵并输出。\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict={x: X, y_: Y})\n",
    "            print(i, total_cross_entropy)\n",
    "    #在训练后神经网络的参数值。\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0193471]\n",
      " [1.0428091]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name='x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name='y-input')\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((2, 1), stddev=1, seed=1))\n",
    "\n",
    "y = tf.matmul(x, w1)\n",
    "\n",
    "loss_less = 10\n",
    "loss_more = 1\n",
    "\n",
    "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_) * loss_more, (y_ - y) * loss_less))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "\n",
    "X = rdm.rand(dataset_size, 2)\n",
    "Y = [[x1 + x2 + rdm.rand()/10.0-0.05] for (x1, x2) in X]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * batch_size) % dataset_size\n",
    "        end = min(start + batch_size, dataset_size)\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFLearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:\\mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting E:\\mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting E:\\mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting E:\\mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    " \n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "trainX, trainY, testX, testY = mnist.load_data(data_dir=\"E:\\mnist\", one_hot=True)\n",
    "# 将图像数据resize成卷积卷积神经网络输入的格式。\n",
    "trainX = trainX.reshape([-1, 28, 28, 1])\n",
    "testX = testX.reshape([-1, 28, 28, 1])\n",
    " \n",
    "# 构建神经网络。\n",
    "net = input_data(shape=[None, 28, 28, 1], name='input')\n",
    "net = conv_2d(net, 32, 5, activation='relu')\n",
    "net = max_pool_2d(net, 2)\n",
    "net = conv_2d(net, 64, 5, activation='relu')\n",
    "net = max_pool_2d(net, 2)\n",
    "net = fully_connected(net, 500, activation='relu')\n",
    "net = fully_connected(net, 10, activation='softmax')\n",
    "# 定义学习任务。指定优化器为sgd，学习率为0.01，损失函数为交叉熵。\n",
    "net = regression(net, optimizer='sgd', learning_rate=0.01, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3bc08c4177bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.fit(trainX, trainY, n_epoch=20,\n\u001b[0;32m      5\u001b[0m           \u001b[0mvalidation_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           show_metric=True)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mTimestamp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total Time used:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestamp2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mTimestamp1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003nh3w\\pycharmprojects\\python.36\\venv\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[1;32m--> 184\u001b[1;33m                                       self.targets)\n\u001b[0m\u001b[0;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\z003nh3w\\pycharmprojects\\python.36\\venv\\lib\\site-packages\\tflearn\\utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[1;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[0;32m    281\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;31m# If a dict is provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "Timestamp1 = time.clock()\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, n_epoch=20,\n",
    "          validation_set=([testX, testY]),\n",
    "          show_metric=True)\n",
    "Timestamp2 = time.clock()\n",
    "print(\"Total Time used:\", Timestamp2 - Timestamp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    " \n",
    "# 通过Keras封装好的API加载MNIST数据。其中trainX就是一个60000 * 28 * 28的数组，\n",
    "# trainY是每一张图片对应的数字。\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data(path='E:\\mnist\\mnist.npz')\n",
    "\n",
    "# 根据对图像编码的格式要求来设置输入层的格式。\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    trainX = trainX.reshape(trainX.shape[0], 1, img_rows, img_cols)\n",
    "    testX = testX.reshape(testX.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    trainX = trainX.reshape(trainX.shape[0], img_rows, img_cols, 1)\n",
    "    testX = testX.reshape(testX.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    \n",
    "trainX = trainX.astype('float32')\n",
    "testX = testX.astype('float32')\n",
    "trainX /= 255.0\n",
    "testX /= 255.0\n",
    " \n",
    "# 将标准答案转化为需要的格式（one-hot编码）。\n",
    "trainY = keras.utils.to_categorical(trainY, num_classes)\n",
    "testY = keras.utils.to_categorical(testY, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "# 定义损失函数、优化函数和评测方法。\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 1.0820 - acc: 0.7379 - val_loss: 0.3119 - val_acc: 0.9035\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.2580 - acc: 0.9229 - val_loss: 0.1890 - val_acc: 0.9458\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1782 - acc: 0.9470 - val_loss: 0.1373 - val_acc: 0.9609\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1398 - acc: 0.9591 - val_loss: 0.1118 - val_acc: 0.9660\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1158 - acc: 0.9660 - val_loss: 0.1106 - val_acc: 0.9664\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1002 - acc: 0.9697 - val_loss: 0.0838 - val_acc: 0.9752\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0895 - acc: 0.9728 - val_loss: 0.0744 - val_acc: 0.9777\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0809 - acc: 0.9758 - val_loss: 0.0729 - val_acc: 0.9775\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0744 - acc: 0.9776 - val_loss: 0.0633 - val_acc: 0.9819\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0683 - acc: 0.9795 - val_loss: 0.0597 - val_acc: 0.9813\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "Test loss: 0.05965520703401417\n",
      "Test accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(testX, testY))\n",
    " \n",
    "# 在测试数据上计算准确率。\n",
    "score = model.evaluate(testX, testY)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
